Intelligent Systems - Fall 2016
2nd Lab Session's Report
By Student A and Student B
Part 1: Design a neural network
Describe how you have computed the network architecture and weights. 
You should describe the network according to the following format: 

Input layer: 2 units, x1 x2
First hidden layer: 2 neurons
	Neuron1: w11 = 1/128, w21=1/70, b=1
	Neuron2: W12 = 1,w22=0,b=0
Output layer: 3 neurons
	Neuron1: w11=-1,w21=0,b=-0.5
	Neuron2:w12=1,w22=-1,b=0.5
 	Neuron3:w12=1,w23=1,b=1.5

Structure of the Neural Network:
 

Of course, it is your task to define the number of layers, the number of neurons per layer, and the exact values for the weights. 
Part 2: Implementation of the MLP simulator
Provide source code and explanations (comments) for both the feedforward() and backpropagation() procedure. 
The feedforward function.
//Computes the output of the MLP for a particular input pattern (pat)
void feedforward(int pat)
{

	//copy pattern into input layer
	for(int i=0; i < layers[0].size; i++) {
		if(i==0) layers[0].o[i] = 1.0; //bias neuron
		else layers[0].o[i] = pattern[pat][i-1]/128.0; //input normalisation required (between 0 and 1)
	}

	// propagate the data through layers 
	for(int i=1; i<nb_layers; i++) {
		
		//at each layer deal with the bias neuron (unit 0)
		int k;
		if(i!=outputlayer()) {
			k=1; 
			layers[i].o[0]=1.0; //bias neuron
		}                     
		else k=0;           // except the output layer since it doesn't have one!

		for(; k<layers[i].size; k++) {
			//
			// BEGIN IMPLEMENTATION
			//
			// compute the activation of neuron k in layer i (weighted sum)
			//initialize the value of activation
			layers[i].u[k] = 0;
			//compute activation
			for(int j=0; j<layers[i-1].size;j++)
			{
				layers[i].u[k]+=layers[i].w[j][k]*layers[i-1].o[j];
			}

			// then compute the output of neuron k in layer i (using the activation function transfer_f())
			layers[i].o[k]=transfer_f(layers[i].u[k]);
			// that's it for the feedforward procedure
			// END IMPLEMENTATION
		}
	}
}

The backpropagation function.
//Implementation of the BackPropagation Algorithm 
// compute and performs the weigth'change according to the current input 
// pattern pat
//
void backpropagation(int pat)
{
	//compute the error and dE_du at the output layer (outputlayer())
	for(int i=0; i < layers[outputlayer()].size; i++) {
		//
		// BEGIN IMPLEMENTATION
		//
		int rms_error;
		// compute the difference between the network ouput and the desired output (pattern[pat][])
		// compute the local contribution to the RMS error (rms_error+=square of the difference computed above)
			rms_error = layers[outputlayer()].o[i]-pattern[pat][i+2];
		// compute the partial derivative of the error with respect to the activation at the output layer (layers[outputlayer()].dE_du[]) using the derivative of the activation function (deriv_transfer_f(layers[].u[]))
			layers[outputlayer()].dE_du[i]=2*rms_error*deriv_transfer_f(transfer_f(layers[outputlayer()].u[i]));
	
		// END IMPLEMENTATION
	}

	//compute the deltas for the remaining layers if any!
	for(int i=nb_layers-2; i>0; i--) {
		for(int j=0; j<layers[i].size; j++) {
			//
			// BEGIN IMPLEMENTATION
			//
			// compute the weighted sum of delta j in hidden layer i   
			int delta_w=0;
			for(int k=0; k<layers[i-1].size; k++)
			{
				delta_w += layers[i+1].dE_du[k]*layers[i+1].w[j][k];
			}

			// compute the partial derivative of the error with respect to the activation at the current layer (layers[i].dE_du[j]) using the derivative of the activation function (deriv_transfer_f(layers[].u[]))
			layers[i].dE_du[j]=delta_w*deriv_transfer_f(transfer_f(layers[i].u[j]));
			// END IMPLEMENTATION

		}
	}

	//compute weight change and update accordingly for all layers
	for(int i=1; i <nb_layers ; i++) {
		for(int j=0; j<layers[i-1].size; j++) {

			int k;
			if(i!=outputlayer()) k=1; //outputlayer doesn't have a bias neuron but the others do
			else k=0;

			for(; k<layers[i].size; k++) {
				//
				// BEGIN IMPLEMENTATION
				// update the weight jk in layer i using the learning rate (learning_rate) and the previously computed layer[].dE_du[] and output layer[].o[]
				layers[i].w[j][k]-=learning_rate*layers[i].dE_du[k]*layers[i].o[j];
				// END IMPLEMENTATION

			}
		}
	}
}

Part 3: Training and Recall experiments
Detail and comment all the experiments achieved: 
•	Network training
      Programs running at the training mode,  MAXCYCLE = 100, RMS_ERROR_THRESHOLD = 0.01, which means the training will end at the 100th CYCLE or when rms_error less than or equal to 0.01.
      After 100 times’ training, we got the result as follows:
 

•	Recall performance (on the training set set30.x1x2rgb and on the recall set set120.x1x2rgb 
Using recall model and set120.x1x2rgb, the result is shown as follows(part of them):
 
And the result of classification is shown as follows:
 
•	The effect of the learning rate, the number of training cycles and the RMS error stopping criterion. Here you could include a graph showing the RMS error as a function of the number of training cycles for different learning rate values. 
1.	Change the learning rate
learning rate	number of training cycles 	RMS error stopping criterion	RMS error
0.5	100	0.01	0.0181033
0.8	100	0.01	0.179776
1	100	0.01	1.179024
1.2	100	0.01	0.178388
2	100	0.01	0.178338
2.	Change the number of training cycles
learning rate	number of training cycles 	RMS error stopping criterion	RMS error
0.5	100	0.01	0.0181033
0.5	50	0.01	0.0192801
0.5	100	0.01	0.0192748
0.5	500	0.01	0.0191805
0.5	1000	0.01	0.0188239
3.	Change the RMS error stopping criterion
 

learning rate	number of training cycles 	RMS error stopping criterion	RMS error
0.5	1000	0.001	0.0191805
0.5	1000	0.01	0.0191033
0.5	1000	0.05	0.0190801
0.5	1000	0.1	0.0190748

